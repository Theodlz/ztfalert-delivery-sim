{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to filtering an alert stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the hackathon closing out ZTF Summer School 2024, your challenge is to create your own alert broker which can consume a stream of alerts and filter for interesting objects.\n",
    "\n",
    "This is a starter notebook for the hackathon. It will show you how to read a simulated ZTF alert stream and give some examples of how you can build your own filtering for those alerts. Previous notebooks from this week, for example \"ztf_alert_filtering\" from Day 1, provide additional examples that you may find useful to apply to this challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Reading the alert stream\n",
    "\n",
    "Note: here we simulate our Kafka consumer by reading the data with an iterator function. Some people will find that their computers can run the true kafka consumer on these couple hundred thousand alerts on the order of minutes, but we use the iterator to ensure feasible timescales for all users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO ADD: count how many files are in data/20240707, ie how many alerts there were on 2024/07/07 that you will be processing in this challenge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to create the iterator\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "def get_iterator(date):\n",
    "    path = pathlib.Path(\"data\") / date\n",
    "\n",
    "    # grab the full list of alert files and create an iterator\n",
    "    alert_files = sorted(list(path.glob(\"*.avro\")))\n",
    "\n",
    "    for file in alert_files:\n",
    "        content = open(file, mode='rb').read()\n",
    "        yield content\n",
    "\n",
    "iterator = get_iterator(\"20240707\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 alerts in 1.368638515472412 seconds\n"
     ]
    }
   ],
   "source": [
    "# run this to get a sample of alerts from the night of 2024/07/07\n",
    "\n",
    "import time\n",
    "\n",
    "def get_n_alerts(alerts_iterator, n_events):\n",
    "    start = time.time()\n",
    "    events = []\n",
    "    try:\n",
    "        for _ in range(n_events):\n",
    "            events.append(next(alerts_iterator))\n",
    "    except StopIteration:\n",
    "        print(\"No more events available.\")\n",
    "    \n",
    "    print(f\"Processed {n_events} alerts in {time.time() - start} seconds\")\n",
    "\n",
    "    return events\n",
    "\n",
    "test_alerts = get_n_alerts(iterator, 1000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO ADD: Get all of the alerts from the night of 2024/07/07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from fastavro import reader\n",
    "\n",
    "def decode_alerts(message):\n",
    "    bytes_io = io.BytesIO(message)\n",
    "    bytes_io.seek(0)\n",
    "    decoded_msg = reader(bytes_io)\n",
    "    return [record for record in decoded_msg]\n",
    "\n",
    "# Assuming test_alerts is a list of Avro messages\n",
    "test_dict = [decode_alerts(alert) for alert in test_alerts]\n",
    "\n",
    "# Now test_dict contains the actual contents of the Avro messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO ADD: Get the id for one of the alerts, and look it up on your choice of ZTF alert broker, for example alerce (https://alerce.online)\n",
    "# you would be very lucky to find something interesting by chance, but this will be a helpful reference once you have designed a filter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Filtering the alert stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells will start with some general suggested steps in building your filter. Some suggested techniques for filtering are:\n",
    "\n",
    "- Make cuts on [alert parameters](https://zwickytransientfacility.github.io/ztf-avro-alert/schema.html) (as you did in ztf_summer_school_2024/lectures/01/ztf_alert_filtering.ipynb)\n",
    "\n",
    "- Use prv_candidates to understand the alert history, and do things like compute the rate that transients are rising and falling (hint: np.polyfit)\n",
    "\n",
    "- Use a classifier trained in a previous notebook or train a new one\n",
    "\n",
    "- Try something new !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Add: it is recommended to start by printing all of the fields in the alert schema, and look up things you don't undertand in the alert parameters\n",
    "# link above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Add: As you did on the first day, one important first step in your filter will be to filter out \"bogus\" alerts. \n",
    "# Science groups actually have a wide range of ways to do this, from a simple cut on drb to much longer filters that use multiple different fields.\n",
    "# Either (a) make a decision on what value to cut on drb at, or (b) design your own set of conditions to removes bogus alert \n",
    "\n",
    "alerts_real = alerts[alerts['candidate.drb'] > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Add: Another good first step of your filter would be to filter out solar system objects. \n",
    "# Hint: try using the fields ssdistnr and magnr to create such a filter. How many objects are you cutting out with this filter?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to start!\n",
    "\n",
    "Reminder: your challenge is to find one or multiple of these objects in with filters that take minimal time to run.\n",
    "\n",
    "1. Supernovae\n",
    "2. AGN\n",
    "3. Variable stars\n",
    "4. Aliens ??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
